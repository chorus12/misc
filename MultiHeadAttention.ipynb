{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d516d89-69f2-4ecf-bca9-05516f425015",
   "metadata": {},
   "source": [
    "# Unit-testing all elements of a Transformer\n",
    "The pupose of this notebook is to unit-test: \n",
    " * Multi-head attention\n",
    " * Point-wise feed-forward network via conv1d\n",
    " * Layer nomalization\n",
    " * Learnable positional embeddings  \n",
    " \n",
    "All that items are building blocks for the transfomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9de9e14e-8b8f-4b45-a0ea-5257b5a8b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98cc8a3-fb43-4b41-9367-1c6b38bbb85a",
   "metadata": {},
   "source": [
    "## Multihead attention \n",
    "[docs](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)  \n",
    "how to create a proper attention mask to attend only to past events and not to the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc922c0-df8d-4d5c-bdea-de0b24f22afb",
   "metadata": {},
   "source": [
    "Credit for the image to https://jalammar.github.io/illustrated-transformer/  \n",
    "![](./transformer_multi-headed_self-attention-recap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f65ca81e-252d-44d5-9400-fa6cae3f2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dcda71ed-c328-402b-91a4-4d94cc1de4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for a toy model\n",
    "d_model = 6\n",
    "num_heads = 1\n",
    "seq_len = 4\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "73092dda-a215-4c0a-949d-1528662bfd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MultiheadAttention(embed_dim, \n",
    "#                             num_heads, \n",
    "#                             dropout=0.0, \n",
    "#                             bias=True, \n",
    "#                             add_bias_kv=False, \n",
    "#                             add_zero_attn=False, \n",
    "#                             kdim=None, \n",
    "#                             vdim=None, \n",
    "#                             batch_first=False, \n",
    "#                             device=None, \n",
    "#                             dtype=None)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4791361b-5ca3-4803-9cc7-29292efbacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input minibatch\n",
    "x = torch.randn((batch_size, seq_len, d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "21893f92-9262-4687-a22f-b43fdf182b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd913ed6-044f-4d06-98aa-b1a43d1297bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a forward pass\n",
    "with torch.no_grad():\n",
    "    attn_output, attn_output_weights = multihead_attn.forward(x, x, x, key_padding_mask=None, need_weights=True, attn_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55c93435-f434-46e7-8392-08e107975bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 6]), torch.Size([2, 4, 4]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.shape, attn_output_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a9b324d-0f35-4ca1-acaa-018b67bea478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output weights must sum to one\n",
    "attn_output_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a80d9053-edde-4ca6-ad57-b7c80b6e2eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1936, 0.2610, 0.2356, 0.3098],\n",
       "         [0.3099, 0.3128, 0.1453, 0.2319],\n",
       "         [0.2948, 0.1864, 0.3057, 0.2130],\n",
       "         [0.3823, 0.3377, 0.0763, 0.2036]],\n",
       "\n",
       "        [[0.3052, 0.2019, 0.2533, 0.2396],\n",
       "         [0.4544, 0.1016, 0.2112, 0.2327],\n",
       "         [0.3193, 0.2103, 0.2090, 0.2614],\n",
       "         [0.4490, 0.0918, 0.2630, 0.1963]]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "97e465a1-d4e1-49ed-8251-7972561bd520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108   in_proj_weight - torch.Size([18, 6])\n",
      "18      in_proj_bias - torch.Size([18])\n",
      "36   out_proj.weight - torch.Size([6, 6])\n",
      "6      out_proj.bias - torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# structure and number of parameters\n",
    "for n, v in multihead_attn.named_parameters() :\n",
    "    print(f\"{v.numel():<4} {n:>15} - {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22937e37-c2e1-498b-97d9-7cd84a1cd6dd",
   "metadata": {},
   "source": [
    "### Make a masked attention  \n",
    "for sequntial recommendation so `j` can attend to `i` only if `j>=i` -> this removes looking into future items `j<i` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "835eee3a-dd15-47cc-be54-66d480a74bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final attention mask matrix is this one\n",
    "# rows are target seq and columns are source seq\n",
    "# how to read the last line - last output item attends to all source/input items\n",
    "# how to read the first line - first output element attends ONLY to itself\n",
    "# when an element is True - it is excluded from attention\n",
    "# True in mask meaning that this element is excluded from the attention\n",
    "attn_mask = torch.ones((seq_len, seq_len), dtype=torch.bool)\n",
    "attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37df59cf-895a-4abf-bb47-17747193b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attn_output, attn_output_weights = multihead_attn.forward(x, x, x, key_padding_mask=None, need_weights=True, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "196ebe71-75e5-4d12-ab44-55549b3ef596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b00f879e-5670-4a48-b298-df7f387ec976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4977, 0.5023, 0.0000, 0.0000],\n",
       "         [0.3746, 0.2369, 0.3885, 0.0000],\n",
       "         [0.3823, 0.3377, 0.0763, 0.2036]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8173, 0.1827, 0.0000, 0.0000],\n",
       "         [0.4323, 0.2848, 0.2829, 0.0000],\n",
       "         [0.4490, 0.0918, 0.2630, 0.1963]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0e493-e112-4d5c-a2ec-1ff0d18188f1",
   "metadata": {},
   "source": [
    "## PointWiseFeedForward via Conv1d\n",
    "in the simplest case, the output value of the layer with input size (N,Cin,L) and output (N,Cout,Lout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4c708dbe-024f-4bb0-b322-ea85efde8e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 6\n",
      "2 4 6\n"
     ]
    }
   ],
   "source": [
    "# note that prints are the same\n",
    "print(*x.shape) # batch-seq_len-d_model\n",
    "print(batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2c7b8650-0329-477d-a03f-b389da62b51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4513,  0.0353,  0.1270,  0.4740,  0.8944,  1.1747],\n",
       "         [-1.3230,  0.1927, -0.1822,  0.0926,  1.0287, -0.4540],\n",
       "         [ 0.6806,  1.1146,  0.4576, -0.6504, -1.3897, -0.1947],\n",
       "         [-1.2598, -0.0638,  0.6872, -2.4544,  1.2562, -0.5079]],\n",
       "\n",
       "        [[ 1.2511, -0.1029, -0.9956,  0.1884, -0.5661, -0.4276],\n",
       "         [-0.4554,  0.5926, -3.0739,  1.1600, -0.6636, -1.0234],\n",
       "         [-0.7723,  0.8463, -0.3214, -0.7221, -0.2912, -1.7728],\n",
       "         [ 1.5076, -0.5536, -3.4129,  0.1765, -0.0653,  0.5672]]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2512f150-17c6-481c-b935-533e8ff8231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4513, -1.3230,  0.6806, -1.2598],\n",
       "         [ 0.0353,  0.1927,  1.1146, -0.0638],\n",
       "         [ 0.1270, -0.1822,  0.4576,  0.6872],\n",
       "         [ 0.4740,  0.0926, -0.6504, -2.4544],\n",
       "         [ 0.8944,  1.0287, -1.3897,  1.2562],\n",
       "         [ 1.1747, -0.4540, -0.1947, -0.5079]],\n",
       "\n",
       "        [[ 1.2511, -0.4554, -0.7723,  1.5076],\n",
       "         [-0.1029,  0.5926,  0.8463, -0.5536],\n",
       "         [-0.9956, -3.0739, -0.3214, -3.4129],\n",
       "         [ 0.1884,  1.1600, -0.7221,  0.1765],\n",
       "         [-0.5661, -0.6636, -0.2912, -0.0653],\n",
       "         [-0.4276, -1.0234, -1.7728,  0.5672]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we transpose the inner matrix so we would have a column per embedding\n",
    "# this is due to conv1d - it mixes data between rows, i.e. samples\n",
    "x.swapaxes(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bf42c64c-10fd-4b0b-bac2-66d981ed1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(m):\n",
    "    \"\"\"\n",
    "    this is the init weight function that sets all convolution parameters to 1 but the second matrix to 2\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    # l = reduce(lambda x,y: x*y, m.weight.data.shape)\n",
    "    l = m.weight.data.numel() # just the number of elements\n",
    "    if type(m) == nn.Conv1d:\n",
    "        m.weight.data = torch.ones(l).to(torch.float).reshape(m.weight.data.shape)\n",
    "        m.weight.data[1] += m.weight.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "050d819b-3057-41fe-83b7-fd589999ad0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6, 1])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we take in the size of d_model and output the same size (that's the stuff in paper SASrec)\n",
    "# in attention is all you need intermediate dim is 4 times larger than d_model\n",
    "# kernel size = 1 makes it a point-wise transformation - remember we have item embeddings in each column and kernel size corresponds to the number of columns\n",
    "# that we aggregate in summation of convolution\n",
    "layer_conv1d = nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=1, bias=False)\n",
    "layer_conv1d.apply(init_weight)\n",
    "layer_conv1d.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1b8e3ccd-6c3d-4b7d-9c70-9c69d4190a11",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[2.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [2.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_conv1d.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1cf6cbc8-33d2-4511-84e8-d6f203b7492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 4])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_1d_out = layer_conv1d(x.swapaxes(-1,-2))\n",
    "conv_1d_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5511b2a6-ef55-491b-ae51-12ef4ff2882a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2543,  4.5085,  2.2543,  2.2543,  2.2543,  2.2543],\n",
       "         [-0.6453, -1.2905, -0.6453, -0.6453, -0.6453, -0.6453],\n",
       "         [ 0.0181,  0.0361,  0.0181,  0.0181,  0.0181,  0.0181],\n",
       "         [-2.3424, -4.6847, -2.3424, -2.3424, -2.3424, -2.3424]],\n",
       "\n",
       "        [[-0.6529, -1.3058, -0.6529, -0.6529, -0.6529, -0.6529],\n",
       "         [-3.4638, -6.9277, -3.4638, -3.4638, -3.4638, -3.4638],\n",
       "         [-3.0335, -6.0670, -3.0335, -3.0335, -3.0335, -3.0335],\n",
       "         [-1.7805, -3.5610, -1.7805, -1.7805, -1.7805, -1.7805]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change back order of axes and move item embeddings to row orientation (to last dim)\n",
    "conv_1d_out.swapaxes(-1,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ef852da4-115f-4e58-828d-87149922ac65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4513,  0.0353,  0.1270,  0.4740,  0.8944,  1.1747],\n",
       "         [-1.3230,  0.1927, -0.1822,  0.0926,  1.0287, -0.4540],\n",
       "         [ 0.6806,  1.1146,  0.4576, -0.6504, -1.3897, -0.1947],\n",
       "         [-1.2598, -0.0638,  0.6872, -2.4544,  1.2562, -0.5079]],\n",
       "\n",
       "        [[ 1.2511, -0.1029, -0.9956,  0.1884, -0.5661, -0.4276],\n",
       "         [-0.4554,  0.5926, -3.0739,  1.1600, -0.6636, -1.0234],\n",
       "         [-0.7723,  0.8463, -0.3214, -0.7221, -0.2912, -1.7728],\n",
       "         [ 1.5076, -0.5536, -3.4129,  0.1765, -0.0653,  0.5672]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and take a note that now for each embedding we have the same pattern <sum(orig_embedding), twice the sum, sum, ..., sum>\n",
    "# that is due to all conv1d weights are 1 except for the 2-nd channel wich is set to 2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "292958fd-a32a-4447-8159-3683330f7f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2543, -0.6453,  0.0181, -2.3424],\n",
       "        [-0.6529, -3.4638, -3.0335, -1.7805]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "404eddd2-92a2-4382-822e-815350ed18c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.2543, 4.5085, 2.2543, 2.2543, 2.2543, 2.2543],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0181, 0.0361, 0.0181, 0.0181, 0.0181, 0.0181],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.relu(conv_1d_out.swapaxes(-1,-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f1f24-0469-43c5-8349-9281b4245f04",
   "metadata": {},
   "source": [
    "## LayerNorm  \n",
    "[docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)  \n",
    "What it does - it normalises data in each sample and multiplies each dimention by `Gamma`(each dim has it's own gamma) and adds `Bias` (again specific bias for each dim) and those a learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0369d77d-988b-482b-9340-7ff0dbb71996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import LayerNorm\n",
    "embedding = torch.randn(batch_size, seq_len, d_model)\n",
    "layer_norm = nn.LayerNorm(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fe0b4052-fdb2-4747-9631-01be13615ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6             weight - torch.Size([6])\n",
      "6               bias - torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "for n, v in layer_norm.named_parameters() :\n",
    "    print(f\"{v.numel():<4} {n:>15} - {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "723db4a8-65a0-4246-ba87-8486ea3131e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1.]), tensor([0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_norm.weight.data, layer_norm.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2d7e1c92-76e2-4ba4-8ef4-a10ac1a21f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2505,  0.2956,  0.5981, -1.6835,  1.3008, -1.9311],\n",
       "        [-1.1474,  0.2873, -0.5039, -0.5448, -1.5466, -0.7329],\n",
       "        [ 1.7583,  1.6505, -0.2722, -0.8209, -0.9714,  0.9023],\n",
       "        [ 1.1841,  0.7555,  0.3598, -0.6615,  0.1244, -0.7045]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b341114e-fb55-4104-b87e-8830221b7192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9779,  0.2477,  0.4790, -1.2658,  1.0164, -1.4552],\n",
       "        [-0.7871,  1.7261,  0.3401,  0.2684, -1.4865, -0.0611],\n",
       "        [ 1.2393,  1.1429, -0.5791, -1.0705, -1.2054,  0.4728],\n",
       "        [ 1.4583,  0.8382,  0.2655, -1.2124, -0.0751, -1.2746]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Activate module\n",
    "with torch.no_grad():\n",
    "    layer_out = layer_norm(embedding)[0]\n",
    "layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ce0bc-0891-421e-9911-d647b9fd3275",
   "metadata": {},
   "source": [
    "####  Calculate same stuff manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8a766b36-9be5-41dd-b536-b554a9b5b7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0282, -0.6981,  0.3744,  0.1763]),\n",
       " tensor([2.0519, 0.3910, 1.4961, 0.5731]))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.mean(dim=-1)[0], embedding.var(dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4e86b3f1-a014-481e-a7f6-67edd42dd7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0282],\n",
       "        [-0.6981],\n",
       "        [ 0.3744],\n",
       "        [ 0.1763]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.mean(dim=-1)[0].unsqueeze(0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "784da8a8-0cfa-41a2-893d-011a35141271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same result as from layer norm\n",
    "manual_layer_norm = (embedding[0] - embedding.mean(dim=-1)[0].unsqueeze(0).T)/(torch.sqrt(embedding.var(dim=-1, unbiased=False)[0].unsqueeze(0).T + layer_norm.eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0bfe0d6e-d7cb-4101-99cc-567007f15245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_layer_norm - layer_out < 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a3a54ac6-9455-48ea-bc0f-4cb1383fc9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.tril(torch.ones((5, 5), dtype=torch.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73d8b2-9085-4e9a-abf9-f2f8e2f1a3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "09fe8dce-beef-416f-afcc-7a83fbc33747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d887b5ab-472b-4230-b75e-b4ff365b7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.tile(torch.arange(0,seq_len), (batch_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "35990611-380f-42c7-8902-dc5c3b28e4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "db0cb234-6fb6-48c3-b164-07d85860367e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [0, 1, 2, 3]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions.expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6ecbb668-6a20-4d6a-84c0-ed4d09339122",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'expand_as'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_415/3155636755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'expand_as'"
     ]
    }
   ],
   "source": [
    "positions.expand_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad1bc1-d941-4602-b2c2-cf24747871d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
